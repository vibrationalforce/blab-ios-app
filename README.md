# BLAB iOS App ü´ß

**Breath ‚Üí Sound ‚Üí Light ‚Üí Consciousness**

[![Swift](https://img.shields.io/badge/Swift-5.9+-orange.svg)](https://swift.org)
[![iOS](https://img.shields.io/badge/iOS-15.0+-blue.svg)](https://developer.apple.com/ios/)
[![License](https://img.shields.io/badge/License-Proprietary-red.svg)](LICENSE)

> Bio-reactive music creation and performance system combining voice, biofeedback, spatial audio, and light control

---

## üöÄ Quick Start (Xcode Handoff)

```bash
cd /Users/michpack/blab-ios-app
open Package.swift  # Opens in Xcode automatically
```

Then in Xcode:
- `Cmd+B` to build
- `Cmd+R` to run in simulator
- `Cmd+U` to run tests

**üìñ For detailed handoff guide:** See **[XCODE_HANDOFF.md](XCODE_HANDOFF.md)**

---

## üìä Project Status

**Current Phase:** Phase 3 Complete & Optimized ‚úÖ
**Last Update:** 2025-10-24
**GitHub:** `vibrationalforce/blab-ios-app`
**Latest Commit:** `65a260f` - API integration complete

### Phase Completion:
- ‚úÖ **Phase 0:** Project Setup & CI/CD (100%)
- ‚úÖ **Phase 1:** Audio Engine Enhancement (85%)
- ‚úÖ **Phase 2:** Visual Engine Upgrade (90%)
- ‚úÖ **Phase 3:** Spatial Audio + Visual + LED (100% ‚ö°)
- ‚è≥ **Phase 4:** Recording & Session System (80%)
- üîµ **Phase 5:** AI Composition Layer (0%)

**Overall MVP Progress:** ~75%

---

## üéØ What is BLAB?

BLAB is an **embodied multimodal music system** that transforms biometric signals (HRV, heart rate, breathing), voice, gestures, and facial expressions into:
- üåä **Spatial Audio** (3D/4D/Fibonacci Field Arrays)
- üé® **Real-time Visuals** (Cymatics, Mandalas, Particles)
- üí° **LED/DMX Lighting** (Push 3, Art-Net)
- üéπ **MIDI 2.0 + MPE** output

### Core Features (Implemented):

#### **Audio System:**
- ‚úÖ Real-time voice processing (AVAudioEngine)
- ‚úÖ FFT frequency detection
- ‚úÖ YIN pitch detection
- ‚úÖ Binaural beat generator (8 brainwave states)
- ‚úÖ Node-based audio graph
- ‚úÖ Multi-track recording

#### **Spatial Audio (Phase 3):**
- ‚úÖ 6 spatial modes: Stereo, 3D, 4D Orbital, AFA, Binaural, Ambisonics
- ‚úÖ Fibonacci sphere distribution
- ‚úÖ Head tracking (CMMotionManager @ 60 Hz)
- ‚úÖ iOS 15+ compatible, iOS 19+ optimized

#### **Visual Engine:**
- ‚úÖ 5 visualization modes: Cymatics, Mandala, Waveform, Spectral, Particles
- ‚úÖ Metal-accelerated rendering
- ‚úÖ Bio-reactive colors (HRV ‚Üí hue)
- ‚úÖ MIDI/MPE parameter mapping

#### **LED/Lighting Control (Phase 3):**
- ‚úÖ Ableton Push 3 (8x8 RGB LED grid, SysEx)
- ‚úÖ DMX/Art-Net (512 channels, UDP)
- ‚úÖ Addressable LED strips (WS2812, RGBW)
- ‚úÖ 7 LED patterns + 6 light scenes
- ‚úÖ Bio-reactive control (HRV ‚Üí LED colors)

#### **Biofeedback:**
- ‚úÖ HealthKit integration (HRV, Heart Rate)
- ‚úÖ HeartMath coherence algorithm
- ‚úÖ Bio-parameter mapping (HRV ‚Üí audio/visual/light)
- ‚úÖ Real-time signal smoothing

#### **Input Modalities:**
- ‚úÖ Voice (microphone + pitch detection)
- ‚úÖ Face tracking (ARKit, 52 blend shapes)
- ‚úÖ Hand gestures (Vision framework)
- ‚úÖ Biometrics (HealthKit)
- ‚úÖ MIDI input

#### **Unified Control System:**
- ‚úÖ 60 Hz control loop
- ‚úÖ Multi-modal sensor fusion
- ‚úÖ Priority-based input resolution
- ‚úÖ Real-time parameter mapping

---

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           UnifiedControlHub (60 Hz Loop)                ‚îÇ
‚îÇ                                                         ‚îÇ
‚îÇ  Bio ‚Üí Gesture ‚Üí Face ‚Üí Voice ‚Üí MIDI 2.0 + MPE        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ               ‚îÇ                ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   Spatial   ‚îÇ  ‚îÇ Visuals  ‚îÇ   ‚îÇ  Lighting  ‚îÇ
    ‚îÇ   Audio     ‚îÇ  ‚îÇ Mapper   ‚îÇ   ‚îÇ Controller ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ               ‚îÇ                ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 3D/4D/AFA   ‚îÇ  ‚îÇ Cymatics ‚îÇ   ‚îÇ  Push 3    ‚îÇ
    ‚îÇ Fibonacci   ‚îÇ  ‚îÇ Mandala  ‚îÇ   ‚îÇ  8x8 LEDs  ‚îÇ
    ‚îÇ Binaural    ‚îÇ  ‚îÇ Particles‚îÇ   ‚îÇ            ‚îÇ
    ‚îÇ Ambisonics  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ DMX/Art-Net‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Components:

1. **UnifiedControlHub** - Central orchestrator (60 Hz control loop)
2. **SpatialAudioEngine** - 3D/4D spatial audio rendering
3. **MIDIToVisualMapper** - MIDI/MPE ‚Üí visual parameter mapping
4. **Push3LEDController** - Ableton Push 3 LED control
5. **MIDIToLightMapper** - DMX/Art-Net lighting control
6. **MIDI2Manager** - MIDI 2.0 protocol implementation
7. **MPEZoneManager** - MPE (MIDI Polyphonic Expression)

---

## üìÅ Project Structure

```
blab-ios-app/
‚îú‚îÄ‚îÄ Package.swift                    # Swift Package config
‚îú‚îÄ‚îÄ Sources/Blab/
‚îÇ   ‚îú‚îÄ‚îÄ BlabApp.swift               # App entry point
‚îÇ   ‚îú‚îÄ‚îÄ ContentView.swift           # Main UI
‚îÇ   ‚îú‚îÄ‚îÄ Audio/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AudioEngine.swift       # Core audio engine
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Effects/               # Audio effects (reverb, filter, etc.)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DSP/                   # DSP (FFT, pitch detection)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Nodes/                 # Modular audio nodes
‚îÇ   ‚îú‚îÄ‚îÄ Spatial/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ SpatialAudioEngine.swift     # 3D/4D spatial audio ‚ú®
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ARFaceTrackingManager.swift  # Face tracking
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ HandTrackingManager.swift    # Hand tracking
‚îÇ   ‚îú‚îÄ‚îÄ Visual/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MIDIToVisualMapper.swift     # MIDI ‚Üí Visual ‚ú®
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CymaticsRenderer.swift       # Cymatics patterns
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Modes/                       # 5 visualization modes
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Shaders/                     # Metal shaders
‚îÇ   ‚îú‚îÄ‚îÄ LED/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Push3LEDController.swift     # Push 3 LED ‚ú®
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ MIDIToLightMapper.swift      # DMX/Art-Net ‚ú®
‚îÇ   ‚îú‚îÄ‚îÄ MIDI/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MIDI2Manager.swift           # MIDI 2.0
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MPEZoneManager.swift         # MPE
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ MIDIToSpatialMapper.swift    # MIDI ‚Üí Spatial
‚îÇ   ‚îú‚îÄ‚îÄ Unified/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ UnifiedControlHub.swift      # Central control ‚ú®
‚îÇ   ‚îú‚îÄ‚îÄ Biofeedback/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ HealthKitManager.swift       # HealthKit
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ BioParameterMapper.swift     # Bio ‚Üí Audio mapping
‚îÇ   ‚îú‚îÄ‚îÄ Recording/                       # Multi-track recording
‚îÇ   ‚îú‚îÄ‚îÄ Views/                           # UI components
‚îÇ   ‚îî‚îÄ‚îÄ Utils/                           # Utilities
‚îú‚îÄ‚îÄ Tests/BlabTests/                     # Unit tests
‚îî‚îÄ‚îÄ Docs/                                # Documentation

‚ú® = Phase 3 components (2228 lines optimized code)
```

---

## üõ†Ô∏è Technical Stack

- **Language:** Swift 5.9+
- **UI:** SwiftUI + Combine
- **Audio:** AVFoundation + CoreAudio
- **Graphics:** Metal + SwiftUI Canvas
- **Biofeedback:** HealthKit + CoreMotion
- **Spatial:** AVAudioEnvironmentNode (iOS 19+)
- **Vision:** ARKit + Vision Framework
- **MIDI:** CoreMIDI + MIDI 2.0
- **Networking:** Network Framework (UDP/Art-Net)
- **Platform:** iOS 15.0+ (optimized for iOS 19+)

---

## üß™ Testing

### Run Tests:
```bash
swift test
# or in Xcode: Cmd+U
```

### Test Coverage:
- **Current:** ~40%
- **Target:** >80%

### Test Suites:
- Audio Engine Tests
- Biofeedback Tests
- Pitch Detection Tests
- Phase 3 Integration Tests (recommended to add)

---

## üìñ Documentation

### Quick References:
- **[XCODE_HANDOFF.md](XCODE_HANDOFF.md)** - Xcode development guide (MUST READ)
- **[PHASE_3_OPTIMIZED.md](PHASE_3_OPTIMIZED.md)** - Phase 3 optimization details
- **[DAW_INTEGRATION_GUIDE.md](DAW_INTEGRATION_GUIDE.md)** - DAW integration
- **[BLAB_IMPLEMENTATION_ROADMAP.md](BLAB_IMPLEMENTATION_ROADMAP.md)** - Full roadmap
- **[BLAB_90_DAY_ROADMAP.md](BLAB_90_DAY_ROADMAP.md)** - 90-day plan

### Additional Docs:
- `COMPATIBILITY.md` - iOS compatibility notes
- `DEPLOYMENT.md` - Deployment guide
- `TESTFLIGHT_SETUP.md` - TestFlight configuration

---

## üé® UI Development

### Recommended Next Steps:

1. **Create Phase 3 Controls:**
   - See `XCODE_HANDOFF.md` Section 4.1 for full code
   - Add spatial audio mode selector
   - Add visual mapping controls
   - Add Push 3 LED pattern picker
   - Add DMX scene selector

2. **Integrate into ContentView:**
   - Add settings/gear button
   - Show Phase3ControlsView in sheet
   - Wire UnifiedControlHub to UI

3. **Add Real-time Displays:**
   - Control loop frequency indicator
   - Bio-signal displays (HRV, HR)
   - Spatial audio source visualization
   - LED pattern preview

---

## ‚öôÔ∏è Configuration

### Info.plist Requirements:
```xml
<!-- Microphone -->
<key>NSMicrophoneUsageDescription</key>
<string>BLAB needs microphone access to process your voice</string>

<!-- Health Data -->
<key>NSHealthShareUsageDescription</key>
<string>BLAB needs access to heart rate data for bio-reactive music</string>

<!-- Camera (for face tracking) -->
<key>NSCameraUsageDescription</key>
<string>BLAB uses face tracking for expressive control</string>
```

### Network Configuration (DMX/Art-Net):
```swift
// Default Art-Net settings
Address: 192.168.1.100
Port: 6454
Universe: 512 channels
```

---

## üöÄ Deployment

### Build for TestFlight:
```bash
# 1. Archive in Xcode
Product ‚Üí Archive

# 2. Upload to App Store Connect
Window ‚Üí Organizer ‚Üí Distribute App

# 3. TestFlight
Invite testers via App Store Connect
```

See `TESTFLIGHT_SETUP.md` for detailed instructions.

---

## üêõ Known Issues & Limitations

### Expected Behaviors:
1. **Simulator:**
   - HealthKit not available ‚Üí Use mock data
   - Push 3 not detected ‚Üí Hardware required
   - Head tracking disabled ‚Üí No motion sensors

2. **Hardware Requirements:**
   - Push 3: USB connection required
   - DMX: Network 192.168.1.100 must be reachable
   - AirPods Pro: For head tracking (iOS 19+)

3. **iOS Versions:**
   - iOS 15-18: Full functionality except iOS 19+ features
   - iOS 19+: AVAudioEnvironmentNode for spatial audio

### TODOs (non-critical):
```swift
// UnifiedControlHub: Calculate breathing rate from HRV
// UnifiedControlHub: Get audio level from audio engine
```

These use fallback values that work fine.

---

## üìä Code Quality Metrics

### Phase 3 Statistics:
- **Total Lines:** 2,228 (optimized)
- **Force Unwraps:** 0 ‚úÖ
- **Compiler Warnings:** 0 ‚úÖ
- **Test Coverage:** ~40% (target: >80%)
- **Documentation:** Comprehensive ‚úÖ

### Performance:
- **Control Loop:** 60 Hz target ‚úÖ
- **CPU Usage:** <30% target
- **Memory:** <200 MB target
- **Frame Rate:** 60 FPS (target 120 FPS on ProMotion)

---

## ü§ù Development Workflow

### Git Workflow:
```bash
# Check status
git status
git log --oneline -5

# Create feature branch
git checkout -b feature/my-feature

# Commit changes
git add .
git commit -m "feat: Add feature description"

# Push to GitHub
git push origin feature/my-feature

# Create PR on GitHub
```

### Commit Convention:
- `feat:` New feature
- `fix:` Bug fix
- `docs:` Documentation
- `refactor:` Code refactoring
- `test:` Tests
- `chore:` Maintenance

---

## üìû Support

### Issues & Questions:
- Open GitHub issue
- Check documentation in `/Docs`
- Review `XCODE_HANDOFF.md`

### Development Team:
- **Original Implementation:** ChatGPT Codex
- **Lead Developer & Optimization:** Claude Code
- **Project Owner:** vibrationalforce

---

## üéØ Roadmap Summary

### ‚úÖ Completed:
- Phase 0: Project Setup
- Phase 1: Audio Engine (85%)
- Phase 2: Visual Engine (90%)
- **Phase 3: Spatial + Visual + LED (100%)** ‚ö°

### ‚è≥ In Progress:
- Phase 4: Recording & Session System (80%)

### üîµ Planned:
- Phase 5: AI Composition Layer
- Phase 6: Networking & Collaboration
- Phase 7: AUv3 Plugin + MPE
- Phase 8: Vision Pro / ARKit
- Phase 9: Distribution & Publishing
- Phase 10: Polish & Release

**Estimated MVP Completion:** 3-4 months
**Full Feature Set:** 6-7 months

See `BLAB_IMPLEMENTATION_ROADMAP.md` for details.

---

## üìú License

Copyright ¬© 2025 BLAB Studio. All rights reserved.

Proprietary software - not for redistribution.

---

## ü´ß Philosophy

> "BLAB is not just a music app - it's an interface to embodied consciousness.
> Through breath, biometrics, and intention, we transform life itself into art."

**breath ‚Üí sound ‚Üí light ‚Üí consciousness**

---

**Built with** ‚ù§Ô∏è using Swift, SwiftUI, AVFoundation, Metal, HealthKit, ARKit, and pure creative energy.

**Status:** ‚úÖ Ready for Xcode Development
**Next:** üöÄ UI Integration & Testing
**Vision:** üåä Embodied Multimodal Music System

ü´ß *Let's flow...* ‚ú®
