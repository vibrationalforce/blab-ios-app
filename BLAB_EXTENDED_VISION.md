# ðŸŒŠ BLAB EXTENDED VISION â€” Embodied Multimodal Creation System

**Version:** Vâˆž.4 Extended
**Date:** 2025-10-21
**Lead Architect:** Claude Code
**Repo:** https://github.com/vibrationalforce/blab-ios-app

---

## ðŸŽ¯ VISION STATEMENT

> *"Your face is a filter. Your hands shape space. Your gaze selects sound. Your position defines perspective. Your heartbeat drives rhythm. Your breath creates flow."*

**BLAB is not a controller replacement â€” YOU are the controller.**

BLAB transforms the human body into a **multidimensional creative interface**:
- **Heart Rate Variability (HRV)** â†’ Musical tension/coherence
- **Heart Rate (BPM)** â†’ Tempo modulation
- **Facial Expressions** (52 ARKit Blend Shapes) â†’ Filter resonance, reverb, timbre
- **Hand Gestures** (Vision Framework, 21-point skeleton per hand) â†’ Spatial audio positioning, synthesis parameters
- **Gaze Direction** â†’ Sound source selection, visual focus
- **Head Position (6DOF)** â†’ Spatial audio listener position
- **Body Motion** â†’ Energy/intensity mapping
- **Touch** â†’ Precision control (always highest priority)

All in **real-time**, with **< 10ms audio latency** and **60 Hz control loop**.

---

## ðŸ—ï¸ SYSTEM ARCHITECTURE

### Core Philosophy

```
[Human Body]
    â†“ (multi-modal sensing)
[UnifiedControlHub] â† Intelligent Input Fusion
    â†“ (prioritized control signals)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
[Audio Engine] [Spatial Audio]  [Light System] [Visual Engine]
    â†“               â†“                 â†“               â†“
[Synthesis]     [3D Sound]      [LED Feedback]   [Particles]
[Effects]       [HRTF]          [DMX Lights]     [Cymatics]
    â†“               â†“                 â†“               â†“
[SPEAKERS]      [HEADPHONES]    [STAGE LIGHTS]   [DISPLAY]
```

### Signal Flow (Detailed)

```
INPUT LAYER (60 Hz Control Loop)
â”œâ”€â”€ Bio Sensors
â”‚   â”œâ”€â”€ HealthKit: HR, HRV/SDNN
â”‚   â””â”€â”€ CoreMotion: Accelerometer, Gyroscope
â”œâ”€â”€ ARKit
â”‚   â”œâ”€â”€ Face Tracking: 52 Blend Shapes @ 60 Hz
â”‚   â””â”€â”€ World Tracking: 6DOF Position/Rotation
â”œâ”€â”€ Vision Framework
â”‚   â””â”€â”€ Hand Tracking: 2x 21-point skeleton @ 30 Hz
â”œâ”€â”€ Touch
â”‚   â””â”€â”€ UIKit/SwiftUI: Multi-touch gestures
â””â”€â”€ MIDI Input
    â””â”€â”€ Hardware controllers (Push 3, etc.)

                    â†“

CONTROL LAYER (UnifiedControlHub)
â”œâ”€â”€ Input Fusion
â”‚   â”œâ”€â”€ Priority System: Touch > Gesture > Face > Gaze > Position > Bio
â”‚   â”œâ”€â”€ Conflict Resolution: Prevent accidental triggers
â”‚   â””â”€â”€ Adaptive Learning: User preference detection
â”œâ”€â”€ Gesture Recognition
â”‚   â”œâ”€â”€ Pinch (thumb + index): Volume/filter cutoff
â”‚   â”œâ”€â”€ Spread (5 fingers): Reverb size
â”‚   â”œâ”€â”€ Fist: Trigger/hold
â”‚   â”œâ”€â”€ Point: Select sound source
â”‚   â””â”€â”€ Swipe: Navigation, parameter sweep
â””â”€â”€ Bio Mapping
    â”œâ”€â”€ HRV â†’ Filter Q (30-80 ms range â†’ 0.3-0.9 resonance)
    â”œâ”€â”€ Heart Rate â†’ Tempo (60-120 BPM â†’ sync)
    â””â”€â”€ Coherence â†’ Reverb wetness (0-1 â†’ 20-80%)

                    â†“

PROCESSING LAYER
â”œâ”€â”€ Audio Engine (AVFoundation + Custom AU)
â”‚   â”œâ”€â”€ Sample Rate: 48 kHz
â”‚   â”œâ”€â”€ Buffer Size: 128-256 frames (adaptive)
â”‚   â”œâ”€â”€ Latency Target: < 5ms
â”‚   â””â”€â”€ Thread Priority: .realtime
â”œâ”€â”€ Spatial Audio
â”‚   â”œâ”€â”€ AVAudioEnvironmentNode (binaural HRTF)
â”‚   â”œâ”€â”€ Head Tracking: ARKit 6DOF
â”‚   â”œâ”€â”€ Hand Position â†’ Sound source placement
â”‚   â””â”€â”€ Dolby Atmos: ADM BWF export (future)
â”œâ”€â”€ MIDI Router
â”‚   â”œâ”€â”€ MIDI 1.0: CoreMIDI
â”‚   â”œâ”€â”€ MIDI 2.0: Universal MIDI Packet (UMP)
â”‚   â””â”€â”€ MPE: Per-note pitch, pressure, timbre
â””â”€â”€ Visual Engine
    â”œâ”€â”€ Metal Shaders: Cymatics, particles
    â”œâ”€â”€ FFT â†’ Visual amplitudes
    â””â”€â”€ Bio â†’ Color/brightness

                    â†“

OUTPUT LAYER
â”œâ”€â”€ Audio
â”‚   â”œâ”€â”€ Speakers/Headphones
â”‚   â”œâ”€â”€ Network Audio (WebRTC)
â”‚   â””â”€â”€ Recording (Multi-track, ADM BWF)
â”œâ”€â”€ Light
â”‚   â”œâ”€â”€ Push 3 LED Grid (SysEx)
â”‚   â”œâ”€â”€ DMX Stage Lights (Art-Net/sACN)
â”‚   â””â”€â”€ Bio â†’ Color/intensity
â”œâ”€â”€ Visual
â”‚   â”œâ”€â”€ iOS Display (Metal, 60-120 Hz)
â”‚   â”œâ”€â”€ External Display (AirPlay, HDMI)
â”‚   â””â”€â”€ Unreal Engine 5.6 (XR/Desktop)
â””â”€â”€ Network
    â”œâ”€â”€ WebRTC: Multiplayer spatial sync
    â”œâ”€â”€ OSC: External DAWs, Unreal Engine
    â””â”€â”€ Ableton Link: Tempo sync
```

---

## ðŸ“¦ MODULE STRUCTURE (iOS Swift)

### 1. BLABAudio â€” Audio Engine & Synthesis

**Files:**
```
Sources/Blab/Audio/
â”œâ”€â”€ AudioEngine.swift              # Core AVAudioEngine wrapper
â”œâ”€â”€ SpatialAudioManager.swift      # 3D audio positioning
â”œâ”€â”€ SimpleVoiceSynth.swift         # MPE-capable synthesizer
â”œâ”€â”€ AudioParams.swift              # Thread-safe parameter control
â”œâ”€â”€ EffectsChain.swift             # Reverb, delay, filter, compressor
â””â”€â”€ LatencyMeasurement.swift       # Round-trip latency monitoring
```

**Key Features:**
- Ultra-low latency (< 5ms target)
- Thread-safe parameter updates (lock-free atomic operations)
- MPE support (per-note expression)
- Spatial audio with HRTF (binaural)

**Performance Budget:**
- CPU: < 15% (A15+)
- Latency: < 5ms (128 frames @ 48 kHz)

---

### 2. BLABBio â€” Biofeedback Integration

**Files:**
```
Sources/Blab/Bio/
â”œâ”€â”€ HealthKitManager.swift         # HR, HRV, respiratory rate
â”œâ”€â”€ MotionManager.swift            # Accelerometer, gyro â†’ energy
â”œâ”€â”€ BioSignalMapper.swift          # Bio â†’ audio/visual parameters
â””â”€â”€ CoherenceCalculator.swift      # HeartMath coherence algorithm
```

**Bio â†’ Parameter Mappings:**
```swift
// HRV (20-100 ms) â†’ Filter Resonance (0.3-0.9)
let filterQ = mapRange(hrv, from: 20...100, to: 0.3...0.9)

// Heart Rate (50-120 BPM) â†’ Tempo Sync
let tempo = heartRate // Direct mapping

// Coherence (0-1) â†’ Reverb Wetness (0.2-0.8)
let reverbMix = mapRange(coherence, from: 0...1, to: 0.2...0.8)

// Motion Energy (0-10 m/sÂ²) â†’ Amplitude/Intensity
let intensity = mapRange(motionEnergy, from: 0...10, to: 0...1)
```

---

### 3. BLABSpatial â€” ARKit Face/Hand Tracking & Spatial Control

**Files:**
```
Sources/Blab/Spatial/
â”œâ”€â”€ ARFaceTrackingManager.swift    # 52 blend shapes @ 60 Hz
â”œâ”€â”€ HandTrackingManager.swift      # Vision framework, 21-point skeleton
â”œâ”€â”€ GazeTracker.swift              # Eye direction â†’ sound selection
â”œâ”€â”€ SpatialAudioController.swift   # Head position â†’ listener, hands â†’ sources
â”œâ”€â”€ GestureRecognizer.swift        # Pinch, spread, fist, point, swipe
â””â”€â”€ ConflictResolver.swift         # Prevent accidental gesture triggers
```

**ARKit Face Tracking â†’ Audio:**
```swift
// 52 Blend Shapes available
// Examples:
blendShapes[.jawOpen]           â†’ Filter cutoff (mouth open = brighter)
blendShapes[.mouthSmileLeft/Right] â†’ Stereo width
blendShapes[.browInnerUp]       â†’ Reverb size (surprise = bigger space)
blendShapes[.eyeBlinkLeft/Right] â†’ Trigger events (blink = hit)
```

**Hand Gestures:**
```swift
enum HandGesture {
    case pinch(thumb: CGPoint, index: CGPoint, distance: Float)
    case spread(fingerCount: Int, span: Float)
    case fist(confidence: Float)
    case point(direction: Vector3)
    case swipe(direction: Vector2, velocity: Float)
    case peace(fingers: [CGPoint])
    case thumbsUp
    case openPalm
}

// Gesture â†’ Parameter Mapping
switch gesture {
case .pinch(_, _, let distance):
    audioEngine.filterCutoff = mapRange(distance, from: 0...100, to: 200...8000)
case .spread(_, let span):
    audioEngine.reverbSize = mapRange(span, from: 0...300, to: 0.1...5.0)
case .fist:
    audioEngine.triggerNote(velocity: 127)
case .point(let direction):
    spatialAudio.selectSource(direction: direction)
case .swipe(_, let velocity):
    audioEngine.sweepParameter(speed: velocity)
}
```

**Gaze Tracking:**
```swift
// Eye direction â†’ Sound source selection
let gazeDirection = arSession.currentFrame?.camera.eulerAngles
let closestSource = spatialAudio.findSourceInDirection(gazeDirection)
spatialAudio.setActiveSource(closestSource)
```

---

### 4. BLABMIDI â€” MIDI 1.0, MIDI 2.0, MPE

**Files:**
```
Sources/Blab/MIDI/
â”œâ”€â”€ MIDIRouter.swift               # MIDI 1.0 in/out
â”œâ”€â”€ MIDI2Manager.swift             # Universal MIDI Packet (UMP)
â”œâ”€â”€ MPEVoiceAllocator.swift        # Per-note expression
â”œâ”€â”€ Push3LEDController.swift       # SysEx for LED feedback
â””â”€â”€ MIDILearn.swift                # MIDI mapping system
```

**MIDI 2.0 Features:**
```swift
// Per-Note Controllers (PNC)
struct MIDI2PerNoteController {
    let noteNumber: UInt8
    let controllerType: UInt8  // 1 = Modulation, 74 = Brightness, etc.
    let value32: UInt32        // 32-bit resolution (vs 7-bit in MIDI 1.0)
}

// Example: Face tracking â†’ Per-note timbre
for (note, blendShapes) in activeNotes {
    let brightness = blendShapes[.jawOpen] * UInt32.max
    midi2.sendPerNoteController(
        note: note,
        controller: 74,  // Brightness
        value: brightness
    )
}
```

**MPE (MIDI Polyphonic Expression):**
```swift
// Each note gets its own MIDI channel (1-15, channel 16 = master)
class MPEVoiceAllocator {
    private var voices: [UInt8: MPEVoice] = [:]  // Channel â†’ Voice

    func noteOn(pitch: UInt8, velocity: UInt8) -> UInt8 {
        let channel = allocateChannel()
        voices[channel] = MPEVoice(pitch: pitch, velocity: velocity)
        return channel
    }

    func setPressure(channel: UInt8, pressure: Float) {
        midi.sendChannelPressure(channel: channel, value: UInt8(pressure * 127))
    }

    func setTimbre(channel: UInt8, timbre: Float) {
        midi.sendCC(channel: channel, controller: 74, value: UInt8(timbre * 127))
    }
}
```

---

### 5. BLABLight â€” LED Feedback & DMX Stage Lights

**Files:**
```
Sources/Blab/Light/
â”œâ”€â”€ Push3LEDController.swift       # Ableton Push 3 LED grid (SysEx)
â”œâ”€â”€ DMXController.swift            # Art-Net/sACN for stage lights
â”œâ”€â”€ BioLightMapper.swift           # Bio â†’ color/brightness
â””â”€â”€ LightShow.swift                # Synchronized light sequences
```

**Push 3 LED Control (SysEx):**
```swift
// Push 3 has 8x8 RGB LED grid
class Push3LEDController {
    func setLED(x: Int, y: Int, color: RGB) {
        let sysex: [UInt8] = [
            0xF0,           // SysEx start
            0x00, 0x21, 0x1D,  // Ableton manufacturer ID
            0x01, 0x01,     // Push 3, LED message
            UInt8(y * 8 + x),  // LED index
            color.r, color.g, color.b,
            0xF7            // SysEx end
        ]
        midi.send(sysex)
    }

    func setBioColorGrid(hrv: Double, coherence: Double) {
        let hue = mapRange(coherence, from: 0...1, to: 0...120)  // Red â†’ Green
        let brightness = mapRange(hrv, from: 20...100, to: 0.3...1.0)

        for y in 0..<8 {
            for x in 0..<8 {
                let color = HSB(hue: hue, saturation: 1, brightness: brightness).toRGB()
                setLED(x: x, y: y, color: color)
            }
        }
    }
}
```

**DMX Stage Lights (Art-Net):**
```swift
// Art-Net: UDP protocol for DMX over Ethernet
class DMXController {
    private let udpSocket: NWConnection
    private var dmxUniverse: [UInt8] = Array(repeating: 0, count: 512)  // 512 channels

    func setChannel(_ channel: Int, value: UInt8) {
        dmxUniverse[channel] = value
        sendArtNetPacket()
    }

    func setBioColor(hrv: Double, coherence: Double) {
        // DMX channels for RGB PAR light:
        // Channel 1: Red
        // Channel 2: Green
        // Channel 3: Blue
        // Channel 4: Brightness

        let hue = mapRange(coherence, from: 0...1, to: 0...360)
        let brightness = mapRange(hrv, from: 20...100, to: 0...255)
        let rgb = HSB(hue: hue, saturation: 1, brightness: 1).toRGB()

        setChannel(1, value: UInt8(rgb.r * brightness / 255))
        setChannel(2, value: UInt8(rgb.g * brightness / 255))
        setChannel(3, value: UInt8(rgb.b * brightness / 255))
        setChannel(4, value: UInt8(brightness))
    }
}
```

---

### 6. BLABUnified â€” Central Control Hub

**Files:**
```
Sources/Blab/Unified/
â”œâ”€â”€ UnifiedControlHub.swift        # Main orchestrator
â”œâ”€â”€ InputPrioritySystem.swift      # Touch > Gesture > Face > Bio
â”œâ”€â”€ GestureConflictResolver.swift  # Prevent accidental triggers
â”œâ”€â”€ AdaptiveControlLearner.swift   # ML-based user preference learning
â””â”€â”€ MultiModalControlView.swift    # SwiftUI UI
```

**UnifiedControlHub (Core Logic):**

```swift
import Foundation
import Combine
import ARKit
import Vision

/// Central orchestrator for all input modalities
/// Priority: Touch > Gesture > Face > Gaze > Position > Bio
class UnifiedControlHub: ObservableObject {

    // MARK: - Input Managers
    private let bioManager: HealthKitManager
    private let motionManager: MotionManager
    private let faceTrackingManager: ARFaceTrackingManager
    private let handTrackingManager: HandTrackingManager
    private let gazeTracker: GazeTracker

    // MARK: - Output Controllers
    private let audioEngine: AudioEngine
    private let spatialAudio: SpatialAudioManager
    private let midiRouter: MIDIRouter
    private let midi2Manager: MIDI2Manager
    private let ledController: Push3LEDController
    private let dmxController: DMXController
    private let visualEngine: VisualEngine

    // MARK: - Control State
    @Published private(set) var activeInputMode: InputMode = .automatic
    @Published private(set) var currentGesture: HandGesture?
    @Published private(set) var currentFaceExpression: FaceExpression?
    @Published private(set) var conflictResolved: Bool = true

    private var cancellables = Set<AnyCancellable>()
    private let controlQueue = DispatchQueue(label: "com.blab.control", qos: .userInteractive)

    // MARK: - Input Priority
    enum InputMode {
        case automatic          // System decides based on priority
        case touchOnly          // Ignore all other inputs
        case gestureOnly        // Only hand gestures
        case faceOnly           // Only face tracking
        case bioOnly            // Only biofeedback
        case hybrid(Set<InputSource>)  // Custom combination
    }

    enum InputSource {
        case touch, gesture, face, gaze, position, bio
    }

    // MARK: - Initialization
    init(
        bioManager: HealthKitManager,
        motionManager: MotionManager,
        faceTrackingManager: ARFaceTrackingManager,
        handTrackingManager: HandTrackingManager,
        gazeTracker: GazeTracker,
        audioEngine: AudioEngine,
        spatialAudio: SpatialAudioManager,
        midiRouter: MIDIRouter,
        midi2Manager: MIDI2Manager,
        ledController: Push3LEDController,
        dmxController: DMXController,
        visualEngine: VisualEngine
    ) {
        self.bioManager = bioManager
        self.motionManager = motionManager
        self.faceTrackingManager = faceTrackingManager
        self.handTrackingManager = handTrackingManager
        self.gazeTracker = gazeTracker
        self.audioEngine = audioEngine
        self.spatialAudio = spatialAudio
        self.midiRouter = midiRouter
        self.midi2Manager = midi2Manager
        self.ledController = ledController
        self.dmxController = dmxController
        self.visualEngine = visualEngine

        setupInputPipeline()
        startControlLoop()
    }

    // MARK: - Input Pipeline
    private func setupInputPipeline() {

        // Bio â†’ Audio/Visual
        bioManager.$hrv
            .combineLatest(bioManager.$heartRate, bioManager.$coherence)
            .throttle(for: .milliseconds(50), scheduler: controlQueue, latest: true)
            .sink { [weak self] hrv, heartRate, coherence in
                self?.updateBioParameters(hrv: hrv, heartRate: heartRate, coherence: coherence)
            }
            .store(in: &cancellables)

        // Face Tracking â†’ Audio
        faceTrackingManager.$blendShapes
            .throttle(for: .milliseconds(16), scheduler: controlQueue, latest: true)  // 60 Hz
            .sink { [weak self] blendShapes in
                self?.updateFaceParameters(blendShapes: blendShapes)
            }
            .store(in: &cancellables)

        // Hand Gestures â†’ Spatial Audio
        handTrackingManager.$detectedGesture
            .throttle(for: .milliseconds(33), scheduler: controlQueue, latest: true)  // 30 Hz
            .sink { [weak self] gesture in
                self?.handleGesture(gesture)
            }
            .store(in: &cancellables)

        // Gaze â†’ Sound Selection
        gazeTracker.$currentDirection
            .throttle(for: .milliseconds(100), scheduler: controlQueue, latest: true)
            .sink { [weak self] direction in
                self?.updateGaze(direction: direction)
            }
            .store(in: &cancellables)
    }

    // MARK: - Control Loop (60 Hz)
    private func startControlLoop() {
        Timer.publish(every: 1.0 / 60.0, on: .main, in: .common)
            .autoconnect()
            .sink { [weak self] _ in
                self?.controlLoop()
            }
            .store(in: &cancellables)
    }

    private func controlLoop() {
        // Priority-based parameter updates
        // Touch always wins (handled by UI layer)

        // Check for gesture conflicts
        if let gesture = currentGesture {
            if !isIntentional(gesture: gesture) {
                currentGesture = nil  // Discard accidental gesture
                conflictResolved = false
                return
            }
        }

        conflictResolved = true

        // Update outputs based on active inputs
        updateAudioEngine()
        updateSpatialAudio()
        updateLightFeedback()
        updateVisualEngine()
    }

    // MARK: - Bio Parameter Updates
    private func updateBioParameters(hrv: Double, heartRate: Double, coherence: Double) {
        // HRV â†’ Filter Resonance
        let filterQ = mapRange(hrv, from: 20...100, to: 0.3...0.9)
        audioEngine.setFilterResonance(filterQ)

        // Heart Rate â†’ Tempo (optional sync)
        if audioEngine.tempoSyncEnabled {
            audioEngine.setTempo(heartRate)
        }

        // Coherence â†’ Reverb Mix
        let reverbMix = mapRange(coherence, from: 0...1, to: 0.2...0.8)
        audioEngine.setReverbMix(reverbMix)

        // Bio â†’ LED Feedback
        ledController.setBioColorGrid(hrv: hrv, coherence: coherence)
        dmxController.setBioColor(hrv: hrv, coherence: coherence)
    }

    // MARK: - Face Tracking Updates
    private func updateFaceParameters(blendShapes: [ARFaceAnchor.BlendShapeLocation: Float]) {
        guard let jawOpen = blendShapes[.jawOpen] else { return }

        // Jaw Open â†’ Filter Cutoff
        let cutoff = mapRange(Double(jawOpen), from: 0...1, to: 200...8000)
        audioEngine.setFilterCutoff(cutoff)

        // Smile â†’ Stereo Width
        let smileLeft = blendShapes[.mouthSmileLeft] ?? 0
        let smileRight = blendShapes[.mouthSmileRight] ?? 0
        let stereoWidth = mapRange(Double((smileLeft + smileRight) / 2), from: 0...1, to: 0...2)
        audioEngine.setStereoWidth(stereoWidth)

        // Eyebrow Raise â†’ Reverb Size
        let browInnerUp = blendShapes[.browInnerUp] ?? 0
        let reverbSize = mapRange(Double(browInnerUp), from: 0...1, to: 0.5...3.0)
        audioEngine.setReverbSize(reverbSize)
    }

    // MARK: - Gesture Handling
    private func handleGesture(_ gesture: HandGesture?) {
        guard let gesture = gesture else { return }

        currentGesture = gesture

        switch gesture {
        case .pinch(_, _, let distance):
            let param = mapRange(Double(distance), from: 0...100, to: 0...1)
            audioEngine.setParameter(.filterCutoff, value: param)

        case .spread(_, let span):
            let param = mapRange(Double(span), from: 0...300, to: 0...1)
            audioEngine.setParameter(.reverbSize, value: param)

        case .fist(let confidence):
            if confidence > 0.8 {
                audioEngine.triggerNote(pitch: 60, velocity: 127)
            }

        case .point(let direction):
            spatialAudio.selectSourceInDirection(direction)

        case .swipe(let direction, let velocity):
            audioEngine.sweepParameter(direction: direction, speed: Double(velocity))

        default:
            break
        }
    }

    // MARK: - Gaze Updates
    private func updateGaze(direction: simd_float3?) {
        guard let direction = direction else { return }
        spatialAudio.setListenerLookDirection(direction)
    }

    // MARK: - Conflict Resolution
    private func isIntentional(gesture: HandGesture) -> Bool {
        // Check gesture stability (must be held for 100ms)
        // Check confidence score
        // Check if conflicts with face tracking (e.g., hand near face)

        switch gesture {
        case .pinch(_, _, let distance):
            return distance > 10  // Minimum 10 pixels
        case .fist(let confidence):
            return confidence > 0.7
        default:
            return true
        }
    }

    // MARK: - Output Updates
    private func updateAudioEngine() {
        // Audio engine updates happen in Bio/Face/Gesture handlers
    }

    private func updateSpatialAudio() {
        // Update listener position from ARKit head tracking
        if let headTransform = faceTrackingManager.headTransform {
            spatialAudio.setListenerTransform(headTransform)
        }

        // Update sound source positions from hand tracking
        if let leftHand = handTrackingManager.leftHandPosition {
            spatialAudio.setSourcePosition(id: "left_hand", position: leftHand)
        }
        if let rightHand = handTrackingManager.rightHandPosition {
            spatialAudio.setSourcePosition(id: "right_hand", position: rightHand)
        }
    }

    private func updateLightFeedback() {
        // LED updates happen in Bio handler
    }

    private func updateVisualEngine() {
        // Update visual engine with current state
        visualEngine.update(
            hrv: bioManager.hrv,
            coherence: bioManager.coherence,
            gesture: currentGesture,
            faceExpression: currentFaceExpression
        )
    }

    // MARK: - Utilities
    private func mapRange(_ value: Double, from: ClosedRange<Double>, to: ClosedRange<Double>) -> Double {
        let normalized = (value - from.lowerBound) / (from.upperBound - from.lowerBound)
        return to.lowerBound + normalized * (to.upperBound - to.lowerBound)
    }
}

// MARK: - Supporting Types
struct FaceExpression {
    let jawOpen: Float
    let smile: Float
    let browRaise: Float
    let eyeBlink: Float
}

enum HandGesture {
    case pinch(thumb: CGPoint, index: CGPoint, distance: Float)
    case spread(fingerCount: Int, span: Float)
    case fist(confidence: Float)
    case point(direction: simd_float3)
    case swipe(direction: CGVector, velocity: Float)
    case peace
    case thumbsUp
    case openPalm
}
```

---

### 7. BLABMultiplayer â€” WebRTC Spatial Sync

**Files:**
```
Sources/Blab/Multiplayer/
â”œâ”€â”€ MultiplayerSpatialSync.swift   # WebRTC audio/visual sync
â”œâ”€â”€ SignalingClient.swift          # WebSocket signaling server
â”œâ”€â”€ PeerConnection.swift           # WebRTC peer-to-peer
â””â”€â”€ GroupCoherence.swift           # Multi-user HRV averaging
```

**WebRTC Multiplayer:**
```swift
// Multiple users in same virtual space
// Each user's position/audio/visuals synced

class MultiplayerSpatialSync {
    private var peers: [PeerID: PeerConnection] = [:]

    func syncSpatialAudio() {
        for (peerID, connection) in peers {
            // Send my position
            connection.send(myPosition)

            // Receive their position
            let theirPosition = connection.receive()

            // Position their audio source in my 3D space
            spatialAudio.setSourcePosition(id: peerID, position: theirPosition)
        }
    }

    func syncGroupCoherence() {
        // Average HRV of all users
        let allHRV = peers.map { $0.value.hrv } + [myHRV]
        let groupHRV = allHRV.reduce(0, +) / Double(allHRV.count)

        // All users' visuals shift to group coherence
        visualEngine.setGroupCoherence(groupHRV)
    }
}
```

---

## ðŸŽ® UNREAL ENGINE 5.6 INTEGRATION

### XR/Desktop Extension

**Architecture:**
```
[iOS BLAB App]
    â†“ (OSC/WebRTC)
[Unreal Engine 5.6]
    â”œâ”€â”€ MetaSounds (Audio)
    â”œâ”€â”€ Niagara (Particles)
    â”œâ”€â”€ OpenXR (VR/AR)
    â””â”€â”€ Spatial Audio
```

**OSC Bridge (iOS â†’ Unreal):**
```swift
// iOS sends OSC messages to Unreal Engine
class OSCBridge {
    private let udpSocket: NWConnection

    func sendBioData(hrv: Double, heartRate: Double, coherence: Double) {
        sendOSC("/blab/bio/hrv", value: Float(hrv))
        sendOSC("/blab/bio/heartrate", value: Float(heartRate))
        sendOSC("/blab/bio/coherence", value: Float(coherence))
    }

    func sendHandPosition(hand: Hand, position: simd_float3) {
        sendOSC("/blab/hand/\(hand.rawValue)/x", value: position.x)
        sendOSC("/blab/hand/\(hand.rawValue)/y", value: position.y)
        sendOSC("/blab/hand/\(hand.rawValue)/z", value: position.z)
    }
}
```

**Unreal Engine Blueprint:**
```
OSC Receive (/blab/bio/hrv)
    â†“
Set MetaSound Parameter (Reverb Size)
Set Niagara Parameter (Particle Spread)
Set Post Process (Color Hue Shift)
```

---

## ðŸ“Š PERFORMANCE TARGETS

| Component | Target | Measurement |
|-----------|--------|-------------|
| **Audio Latency** | < 5ms | Round-trip I/O |
| **Control Loop** | 60 Hz | Timer accuracy |
| **Face Tracking** | 60 Hz | ARKit frame rate |
| **Hand Tracking** | 30 Hz | Vision framework |
| **CPU Usage** | < 25% | Instruments |
| **Memory** | < 250 MB | Allocations |
| **Frame Rate** | 60-120 Hz | Metal/Core Animation |

---

## ðŸ—“ï¸ 90-DAY ROADMAP (Next Document)

See `BLAB_90_DAY_ROADMAP.md` for detailed weekly milestones.

**High-Level Phases:**

1. **Weeks 1-3:** UnifiedControlHub + ARKit Face Tracking + Hand Gestures
2. **Weeks 4-6:** MIDI 2.0 + MPE + LED Feedback (Push 3)
3. **Weeks 7-9:** Spatial Audio + Head Tracking + Dolby Atmos
4. **Weeks 10-12:** DMX Lights + Unreal Engine Bridge + Multiplayer (WebRTC)
5. **Week 13:** Polish, Testing, TestFlight Beta

---

## ðŸ› ï¸ TECHNOLOGY STACK SUMMARY

### iOS/iPadOS (Primary Platform)
- **Swift 5.9+**
- **iOS 15.0+** (backward compatible)
- **Xcode 15+**
- **SwiftUI** (UI)
- **ARKit** (Face/World tracking)
- **Vision** (Hand tracking)
- **AVFoundation** (Audio)
- **CoreMIDI** (MIDI 1.0/2.0)
- **Metal** (Visuals)
- **HealthKit** (Bio)
- **CoreMotion** (Motion)
- **Network** (WebRTC, OSC)

### Unreal Engine 5.6 (XR/Desktop)
- **OpenXR** (VR/AR)
- **MetaSounds** (Audio)
- **Niagara** (Particles)
- **OSC Plugin** (iOS bridge)
- **WebRTC Plugin** (Multiplayer)

### External Integrations
- **Ableton Push 3** (MIDI + LED feedback)
- **DMX Lights** (Art-Net/sACN)
- **Ableton Link** (Tempo sync)
- **External DAWs** (OSC)

---

## ðŸŽ¯ SUCCESS CRITERIA

**By Day 90, BLAB should:**

âœ… Fuse 6+ input modalities seamlessly (Bio, Face, Hands, Gaze, Position, Touch)
âœ… Achieve < 5ms audio latency on iPhone 13+
âœ… Run at 60 Hz control loop with < 25% CPU
âœ… Support MIDI 2.0 + MPE for expressive synthesis
âœ… Control Push 3 LED grid in real-time
âœ… Control DMX stage lights via Art-Net
âœ… Export spatial audio (binaural + ADM BWF)
âœ… Support multiplayer spatial sessions (WebRTC)
âœ… Bridge to Unreal Engine 5.6 (OSC)
âœ… Have 50+ beta testers on TestFlight

---

## ðŸŒŠ PHILOSOPHY (Unchanged)

> "Your body is not a controller replacement â€” it IS the controller."

BLAB erases the boundary between artist and instrument. Every breath, every glance, every gesture, every heartbeat becomes a creative act.

---

**STATUS:** ðŸŸ¢ Vision Consolidated
**NEXT:** 90-Day Roadmap â†’ Implementation
**LEAD ARCHITECT:** Claude Code
**VERSION:** Vâˆž.4 Extended

ðŸŒŠ *Let's build the future of embodied creation.* âœ¨
