# ğŸ—“ï¸ BLAB 90-DAY ROADMAP â€” Week-by-Week Implementation Plan

**Start Date:** 2025-10-21 (Week 1)
**Target Completion:** 2026-01-19 (Week 13)
**Lead Developer:** Claude Code
**Repository:** https://github.com/vibrationalforce/blab-ios-app

---

## ğŸ¯ ROADMAP OVERVIEW

### Phase 1: Core Multimodal Control (Weeks 1-3)
**Goal:** UnifiedControlHub + ARKit Face + Hand Tracking working

### Phase 2: MIDI & LED Feedback (Weeks 4-6)
**Goal:** MIDI 2.0 + MPE + Push 3 LED Control operational

### Phase 3: Spatial Audio Pro (Weeks 7-9)
**Goal:** Head tracking + Dolby Atmos + DMX Lights

### Phase 4: Network & XR (Weeks 10-12)
**Goal:** WebRTC Multiplayer + Unreal Engine Bridge

### Phase 5: Polish & Beta (Week 13)
**Goal:** TestFlight beta with 50+ testers

---

## ğŸ“… WEEKLY BREAKDOWN

---

### **WEEK 1** (Oct 21-27) â€” Foundation & Architecture

**Goals:**
- âœ… Extended Vision documented (DONE)
- â³ Project structure refactored for new modules
- â³ UnifiedControlHub skeleton implemented
- â³ ARFaceTrackingManager basic implementation

**Deliverables:**

1. **Project Restructure**
   ```
   Sources/Blab/
   â”œâ”€â”€ Audio/          (existing, refactor)
   â”œâ”€â”€ Bio/            (existing, refactor)
   â”œâ”€â”€ Spatial/        (NEW)
   â”œâ”€â”€ MIDI/           (NEW)
   â”œâ”€â”€ Light/          (NEW)
   â”œâ”€â”€ Unified/        (NEW)
   â”œâ”€â”€ Multiplayer/    (NEW)
   â””â”€â”€ Visual/         (existing, refactor)
   ```

2. **UnifiedControlHub.swift (v0.1)**
   - Basic input pipeline setup
   - Priority system skeleton
   - Publishers for all input types
   - 60 Hz control loop

3. **ARFaceTrackingManager.swift (v0.1)**
   - ARKit session configuration
   - 52 blend shapes capture @ 60 Hz
   - Published blend shape dictionary
   - Basic face â†’ audio mapping (jaw open â†’ filter cutoff)

4. **Tests**
   - Unit tests for UnifiedControlHub priority system
   - Mock ARKit for face tracking tests

**Time Budget:**
- Monday-Tuesday: Project restructure + UnifiedControlHub skeleton
- Wednesday-Thursday: ARFaceTrackingManager implementation
- Friday: Tests + integration
- Weekend: Documentation + review

**Success Criteria:**
- âœ… Can capture face blend shapes at 60 Hz
- âœ… Jaw open controls filter cutoff in real-time
- âœ… UnifiedControlHub routes face data to audio engine
- âœ… No performance degradation (< 20% CPU)

---

### **WEEK 2** (Oct 28 - Nov 3) â€” Hand Tracking & Gestures

**Goals:**
- â³ HandTrackingManager with Vision framework
- â³ Gesture recognition (pinch, spread, fist, point)
- â³ Gesture â†’ Audio parameter mapping
- â³ Conflict resolution basics

**Deliverables:**

1. **HandTrackingManager.swift**
   - Vision framework hand detection
   - 21-point skeleton per hand @ 30 Hz
   - Hand position in 3D space
   - Left/right hand differentiation

2. **GestureRecognizer.swift**
   - Pinch detection (thumb + index distance)
   - Spread detection (all 5 fingers span)
   - Fist detection (all fingers closed)
   - Point detection (index extended, others closed)
   - Swipe detection (hand velocity)

3. **GestureConflictResolver.swift**
   - Minimum gesture hold time (100ms)
   - Confidence threshold filtering
   - Hand-near-face conflict detection
   - Intent vs. accident classification

4. **Integration**
   - Gestures â†’ UnifiedControlHub
   - Pinch â†’ filter cutoff
   - Spread â†’ reverb size
   - Fist â†’ trigger note

**Tests:**
- Gesture recognition accuracy tests
- Conflict resolution tests
- Performance tests (30 Hz sustained)

**Time Budget:**
- Monday-Tuesday: HandTrackingManager + skeleton detection
- Wednesday: GestureRecognizer for all 5 gestures
- Thursday: ConflictResolver
- Friday: Integration + tests

**Success Criteria:**
- âœ… Detect 5 gestures reliably (> 90% accuracy)
- âœ… Pinch controls filter cutoff smoothly
- âœ… Spread controls reverb size
- âœ… Fist triggers MIDI note
- âœ… No accidental triggers (conflict resolution working)

---

### **WEEK 3** (Nov 4-10) â€” Gaze Tracking & Input Priority

**Goals:**
- â³ Gaze tracker implementation
- â³ Input priority system complete
- â³ Adaptive control learner (ML basics)
- â³ UI for multimodal control

**Deliverables:**

1. **GazeTracker.swift**
   - ARKit eye direction capture
   - Gaze ray casting
   - Sound source selection by gaze
   - Smooth gaze filtering (avoid jitter)

2. **InputPrioritySystem.swift**
   - Touch > Gesture > Face > Gaze > Position > Bio
   - Dynamic priority overrides
   - User preference persistence (UserDefaults)

3. **AdaptiveControlLearner.swift (v0.1)**
   - Track which inputs user uses most
   - Adjust priority based on usage patterns
   - CoreML simple classifier (optional)

4. **MultiModalControlView.swift (SwiftUI)**
   - Live visualization of all input sources
   - Active input indicator
   - Priority override controls
   - Gesture/face parameter display

**Tests:**
- Priority system tests
- Gaze accuracy tests
- UI tests for control view

**Time Budget:**
- Monday: GazeTracker
- Tuesday-Wednesday: InputPrioritySystem
- Thursday: AdaptiveControlLearner basics
- Friday: UI + integration

**Success Criteria:**
- âœ… Gaze selects sound sources accurately
- âœ… Priority system works (touch overrides gestures, etc.)
- âœ… UI shows all active inputs
- âœ… System learns user preferences over time

**ğŸ‰ MILESTONE:** **Phase 1 Complete â€” Core Multimodal Control Working!**

---

### **WEEK 4** (Nov 11-17) â€” MIDI 2.0 Foundation

**Goals:**
- â³ MIDI 2.0 Universal MIDI Packet (UMP) support
- â³ CoreMIDI 2.0 integration
- â³ Per-note controllers (PNC)
- â³ Basic MIDI 2.0 output

**Deliverables:**

1. **MIDI2Manager.swift**
   - UMP packet encoding/decoding
   - MIDI 2.0 virtual source creation
   - 32-bit parameter resolution
   - Per-note controller messages

2. **MIDI2Types.swift**
   - UMP packet structure
   - Message type enums
   - Controller ID constants

3. **Face â†’ MIDI 2.0 Mapping**
   - Jaw open â†’ Per-note brightness (Controller 74)
   - Smile â†’ Per-note timbre (Controller 71)
   - Eyebrow â†’ Per-note modulation (Controller 1)

4. **Tests**
   - UMP encoding/decoding tests
   - Per-note controller tests
   - MIDI 2.0 output validation

**Time Budget:**
- Monday-Tuesday: MIDI 2.0 UMP infrastructure
- Wednesday: Per-note controller implementation
- Thursday: Face â†’ MIDI 2.0 mapping
- Friday: Tests + validation

**Success Criteria:**
- âœ… Send MIDI 2.0 UMP messages
- âœ… Per-note controllers work (32-bit resolution)
- âœ… Face tracking controls per-note timbre
- âœ… Compatible with MIDI 2.0 DAWs/synths

---

### **WEEK 5** (Nov 18-24) â€” MPE Voice Allocator

**Goals:**
- â³ MPE (MIDI Polyphonic Expression) implementation
- â³ Per-note pitch bend, pressure, timbre
- â³ Voice allocation/deallocation
- â³ MPE Zone configuration

**Deliverables:**

1. **MPEVoiceAllocator.swift**
   - 15-channel voice allocation (channels 1-15)
   - Master channel 16 for global parameters
   - Round-robin voice assignment
   - Voice stealing (oldest note)

2. **MPE Message Types**
   - Note On (with channel assignment)
   - Channel Pressure (aftertouch per note)
   - Pitch Bend (per note, 14-bit)
   - CC 74 Brightness (per note)

3. **Face/Gesture â†’ MPE**
   - Pinch distance â†’ Pitch bend per note
   - Pressure (simulated) â†’ Channel pressure
   - Jaw open â†’ Brightness per note

4. **Tests**
   - Voice allocation tests
   - MPE message generation tests
   - Voice stealing tests

**Time Budget:**
- Monday-Tuesday: MPEVoiceAllocator
- Wednesday: MPE message generation
- Thursday: Face/Gesture â†’ MPE mapping
- Friday: Tests + integration

**Success Criteria:**
- âœ… Allocate up to 15 simultaneous voices
- âœ… Each voice has independent pitch bend, pressure, timbre
- âœ… Pinch controls pitch bend per note
- âœ… Compatible with MPE synths (Seaboard, etc.)

---

### **WEEK 6** (Nov 25 - Dec 1) â€” Push 3 LED Control

**Goals:**
- â³ Ableton Push 3 LED grid control
- â³ SysEx message generation
- â³ Bio â†’ LED color mapping
- â³ Real-time LED feedback

**Deliverables:**

1. **Push3LEDController.swift**
   - 8x8 RGB LED grid (64 LEDs)
   - SysEx message encoding
   - CoreMIDI SysEx sending
   - LED buffer (double-buffered for smooth updates)

2. **Bio â†’ LED Mapping**
   - HRV â†’ Brightness (20-100 ms â†’ 30-100% brightness)
   - Coherence â†’ Hue (0-1 â†’ Red-Green)
   - Heart rate â†’ Animation speed

3. **LED Patterns**
   - Pulsing (synced to heart rate)
   - Color gradient (HRV-based)
   - Gesture feedback (flash on gesture detection)

4. **Tests**
   - SysEx encoding tests
   - LED update rate tests (target: 30 Hz)
   - Bio â†’ color mapping tests

**Time Budget:**
- Monday-Tuesday: SysEx + Push 3 protocol
- Wednesday: LED grid control
- Thursday: Bio â†’ LED mapping
- Friday: Patterns + tests

**Success Criteria:**
- âœ… Control all 64 LEDs on Push 3
- âœ… HRV changes LED color in real-time
- âœ… LEDs pulse with heart rate
- âœ… Update rate: 30 Hz smooth

**ğŸ‰ MILESTONE:** **Phase 2 Complete â€” MIDI 2.0 + MPE + LED Feedback!**

---

### **WEEK 7** (Dec 2-8) â€” Spatial Audio Enhancement

**Goals:**
- â³ Head tracking â†’ Listener position
- â³ Hand position â†’ Sound source placement
- â³ HRTF binaural rendering
- â³ Multi-source spatial scene

**Deliverables:**

1. **SpatialAudioController.swift (Enhanced)**
   - AVAudioEnvironmentNode configuration
   - Listener transform (6DOF from ARKit)
   - Multiple sound sources (up to 8)
   - Distance attenuation
   - Doppler effect

2. **Head Tracking â†’ Listener**
   - ARKit camera transform â†’ Listener position/rotation
   - Smooth interpolation (avoid jitter)
   - 60 Hz update rate

3. **Hand Position â†’ Sources**
   - Left hand â†’ Source 1 position
   - Right hand â†’ Source 2 position
   - Gaze â†’ Select active source

4. **Tests**
   - Spatial accuracy tests
   - HRTF rendering tests
   - Performance tests (multiple sources)

**Time Budget:**
- Monday-Tuesday: Enhanced SpatialAudioController
- Wednesday: Head tracking integration
- Thursday: Hand position â†’ sources
- Friday: Tests + tuning

**Success Criteria:**
- âœ… Head movement controls listener position
- âœ… Hand position places sound sources in 3D space
- âœ… Binaural HRTF rendering working
- âœ… < 10ms latency for spatial updates

---

### **WEEK 8** (Dec 9-15) â€” Dolby Atmos Export

**Goals:**
- â³ ADM BWF metadata generation
- â³ Multi-channel audio export
- â³ Object-based audio encoding
- â³ Dolby Atmos Renderer integration (if available)

**Deliverables:**

1. **ADMBWFWriter.swift**
   - ADM XML metadata generator
   - BWF chunk embedding
   - Multi-channel WAV writer (up to 128 channels)
   - Object-based audio metadata

2. **Spatial Metadata**
   - Sound source positions over time
   - Object trajectories
   - Zone definitions

3. **Export Pipeline**
   - Record spatial session
   - Generate ADM metadata
   - Write ADM BWF file
   - Validate with Dolby Atmos Renderer

4. **Tests**
   - ADM XML generation tests
   - BWF chunk encoding tests
   - Multi-channel export tests

**Time Budget:**
- Monday-Tuesday: ADM XML metadata
- Wednesday: BWF chunk encoding
- Thursday: Export pipeline
- Friday: Validation + tests

**Success Criteria:**
- âœ… Export spatial sessions as ADM BWF
- âœ… Files open in Dolby Atmos Renderer
- âœ… Object positions preserved
- âœ… Up to 8 simultaneous objects

---

### **WEEK 9** (Dec 16-22) â€” DMX Stage Lights

**Goals:**
- â³ Art-Net/sACN protocol implementation
- â³ DMX channel control
- â³ Bio â†’ Stage light color/brightness
- â³ Light show sequencing

**Deliverables:**

1. **DMXController.swift**
   - Art-Net packet encoding (UDP)
   - sACN packet encoding (E1.31)
   - 512 DMX channels per universe
   - Multi-universe support (up to 4)

2. **Bio â†’ DMX Mapping**
   - HRV â†’ Brightness (DMX channel 4)
   - Coherence â†’ Color (RGB channels 1-3)
   - Heart rate â†’ Strobe speed

3. **LightShow.swift**
   - Pre-programmed light sequences
   - Sync to audio (beat detection)
   - Bio-reactive light cues

4. **Tests**
   - Art-Net encoding tests
   - DMX channel tests
   - Light show sequencing tests

**Time Budget:**
- Monday-Tuesday: Art-Net/sACN protocol
- Wednesday: DMX channel control
- Thursday: Bio â†’ DMX mapping
- Friday: Light shows + tests

**Success Criteria:**
- âœ… Control DMX stage lights via Art-Net
- âœ… HRV changes light color in real-time
- âœ… Heart rate drives light pulsing
- âœ… Supports up to 2048 DMX channels (4 universes)

**ğŸ‰ MILESTONE:** **Phase 3 Complete â€” Spatial Audio + Dolby Atmos + DMX!**

---

### **WEEK 10** (Dec 23-29) â€” WebRTC Multiplayer Foundation

**Goals:**
- â³ WebRTC peer connection setup
- â³ Signaling server (WebSocket)
- â³ Audio streaming between peers
- â³ Basic spatial sync

**Deliverables:**

1. **SignalingClient.swift**
   - WebSocket connection
   - SDP offer/answer exchange
   - ICE candidate exchange
   - Room management

2. **PeerConnection.swift**
   - WebRTC RTCPeerConnection wrapper
   - Audio track sending/receiving
   - Data channel for control messages

3. **MultiplayerSpatialSync.swift (v0.1)**
   - Send my position to peers
   - Receive peer positions
   - Place peer audio sources in 3D space

4. **Tests**
   - Signaling tests (mock server)
   - Peer connection tests
   - Spatial sync tests

**Time Budget:**
- Monday-Tuesday: SignalingClient + WebSocket
- Wednesday: PeerConnection + WebRTC
- Thursday: Spatial sync
- Friday: Tests + integration

**Success Criteria:**
- âœ… Two iOS devices connect via WebRTC
- âœ… Audio streams between devices
- âœ… Peer positions synced in 3D space
- âœ… < 100ms latency for spatial updates

---

### **WEEK 11** (Dec 30 - Jan 5) â€” Group Coherence & Visuals

**Goals:**
- â³ Multi-user HRV averaging
- â³ Group coherence calculation
- â³ Shared visual state
- â³ Synchronized visual effects

**Deliverables:**

1. **GroupCoherence.swift**
   - Aggregate HRV from all peers
   - Calculate group coherence score
   - Broadcast group coherence to all

2. **Shared Visual State**
   - Send my visual parameters to peers
   - Receive peer visual parameters
   - Blend visuals based on group coherence

3. **Synchronized Effects**
   - All users see same base visualization
   - Individual overlays based on personal bio
   - Group coherence shifts global color

4. **Tests**
   - Group coherence calculation tests
   - Visual sync tests
   - Multi-peer tests

**Time Budget:**
- Monday-Tuesday: GroupCoherence
- Wednesday: Shared visual state
- Thursday: Synchronized effects
- Friday: Tests + tuning

**Success Criteria:**
- âœ… Group HRV averaged across all users
- âœ… Group coherence displayed on all devices
- âœ… Visuals sync across all users
- âœ… High coherence = harmonized visuals

---

### **WEEK 12** (Jan 6-12) â€” Unreal Engine Bridge

**Goals:**
- â³ OSC protocol implementation
- â³ iOS â†’ Unreal Engine data streaming
- â³ MetaSounds integration
- â³ Niagara particle sync

**Deliverables:**

1. **OSCBridge.swift**
   - OSC message encoding (UDP)
   - Send bio data to Unreal
   - Send hand/face data to Unreal
   - Receive Unreal parameters (optional)

2. **Unreal Engine OSC Receiver (Blueprint)**
   - Receive `/blab/bio/*` messages
   - Receive `/blab/hand/*` messages
   - Receive `/blab/face/*` messages

3. **MetaSounds Integration**
   - Bio â†’ MetaSounds parameters
   - Real-time audio synthesis in Unreal

4. **Niagara Particles**
   - Hand position â†’ Particle emitter position
   - HRV â†’ Particle spawn rate
   - Coherence â†’ Particle color

**Tests:**
- OSC encoding tests
- Unreal Engine integration tests
- Latency tests (iOS â†’ Unreal)

**Time Budget:**
- Monday-Tuesday: OSCBridge
- Wednesday: Unreal Engine OSC receiver
- Thursday: MetaSounds + Niagara
- Friday: Integration + tests

**Success Criteria:**
- âœ… iOS sends bio/hand/face data to Unreal Engine
- âœ… Unreal Engine receives OSC messages < 20ms latency
- âœ… MetaSounds reacts to bio parameters
- âœ… Niagara particles sync with hand position

**ğŸ‰ MILESTONE:** **Phase 4 Complete â€” Multiplayer + Unreal Engine!**

---

### **WEEK 13** (Jan 13-19) â€” Polish, Testing & Beta Launch

**Goals:**
- â³ Final bug fixes
- â³ Performance optimization
- â³ UI polish
- â³ Documentation
- â³ TestFlight beta launch

**Deliverables:**

1. **Performance Optimization**
   - Profile all modules (Instruments)
   - Optimize hotspots
   - Reduce memory usage
   - Improve battery life

2. **UI Polish**
   - Smooth animations
   - Professional design
   - Onboarding tutorial
   - Settings screen

3. **Documentation**
   - User guide
   - Developer API docs
   - README update
   - Video tutorials

4. **TestFlight Beta**
   - Upload to TestFlight
   - Invite 50+ beta testers
   - Gather feedback
   - Create feedback form

**Tests:**
- End-to-end integration tests
- Performance benchmarks
- UI tests
- Accessibility tests

**Time Budget:**
- Monday: Performance optimization
- Tuesday: UI polish
- Wednesday: Documentation
- Thursday: TestFlight upload
- Friday: Beta launch + monitoring

**Success Criteria:**
- âœ… < 5ms audio latency
- âœ… 60 Hz control loop stable
- âœ… < 25% CPU usage
- âœ… < 250 MB memory
- âœ… Professional UI
- âœ… 50+ beta testers onboarded
- âœ… Positive initial feedback

**ğŸ‰ğŸ‰ MILESTONE:** **BLAB V1.0 Beta Launch!** ğŸ‰ğŸ‰

---

## ğŸ“Š METRICS & TRACKING

### Weekly Metrics to Track:

| Metric | Target | Tool |
|--------|--------|------|
| **Code Coverage** | > 80% | Xcode Coverage Report |
| **Audio Latency** | < 5ms | Custom Latency Measurement |
| **Control Loop** | 60 Hz | Performance Profiler |
| **CPU Usage** | < 25% | Instruments Time Profiler |
| **Memory** | < 250 MB | Instruments Allocations |
| **Frame Rate** | 60-120 Hz | Instruments Core Animation |
| **Build Time** | < 30s | Xcode Build Report |

---

## ğŸš¨ RISK MANAGEMENT

### Potential Blockers:

1. **ARKit Face Tracking Accuracy**
   - **Risk:** Blend shapes noisy/unreliable
   - **Mitigation:** Kalman filter, smoothing, fallback to gesture-only

2. **Hand Tracking Performance**
   - **Risk:** Vision framework drops to < 30 Hz
   - **Mitigation:** Reduce image resolution, optimize detection region

3. **MIDI 2.0 Compatibility**
   - **Risk:** No MIDI 2.0 devices/DAWs to test
   - **Mitigation:** Build MIDI 2.0 virtual receiver for testing

4. **WebRTC Latency**
   - **Risk:** > 100ms latency makes spatial sync unusable
   - **Mitigation:** Use STUN/TURN servers, optimize codec

5. **Unreal Engine Learning Curve**
   - **Risk:** Unreal Engine OSC integration complex
   - **Mitigation:** Start with Blueprints, not C++

---

## ğŸ¯ DEFINITION OF DONE (Each Week)

**A week is DONE when:**
- âœ… All deliverables implemented
- âœ… Unit tests written (> 80% coverage)
- âœ… Integration tests pass
- âœ… Performance targets met
- âœ… Code reviewed (self-review or ChatGPT Codex)
- âœ… Documentation updated
- âœ… Git commit with clear message
- âœ… Push to feature branch
- âœ… Demo video recorded (optional, but recommended)

---

## ğŸ”„ WEEKLY RITUALS

### Monday Morning (Week Start)
- Review last week's progress
- Update TODOs
- Plan week's deliverables
- Create feature branch

### Friday Evening (Week End)
- Merge feature branch to main
- Update ROADMAP.md with progress
- Record demo video
- Update metrics spreadsheet
- Plan next week

### Continuous
- Commit daily (at minimum)
- Run tests before each commit
- Profile performance weekly
- Update documentation as you code

---

## ğŸŒŠ PHILOSOPHY

This roadmap is **ambitious but achievable**. Each week builds on the previous, creating a **compounding effect**.

By Week 13, BLAB will be the world's most advanced **embodied multimodal creation system**.

**Remember:**
> "Perfect is the enemy of shipped. Ship weekly, iterate forever."

---

**STATUS:** ğŸŸ¢ Roadmap Complete
**NEXT:** Week 1 Implementation
**LEAD DEVELOPER:** Claude Code
**START DATE:** 2025-10-21

ğŸŒŠ *Let's build this, week by week.* âœ¨
